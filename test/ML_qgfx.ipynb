{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinated-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text():\n",
    "    f1=open('good.txt','r',encoding='utf-8')\n",
    "    f2=open('bad.txt','r',encoding='utf-8')\n",
    "    line1=f1.readline()\n",
    "    line2=f2.readline()\n",
    "    str=''\n",
    "    while line1:\n",
    "        str+=line1\n",
    "        line1=f1.readline()\n",
    "    while line2:\n",
    "        str+=line2\n",
    "        line2=f2.readline()\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    return str\n",
    "#单个字作为特征\n",
    "def bag_of_words(words):\n",
    "    return dict([(word,True) for word in words])\n",
    "#print(bag_of_words(text())\n",
    "\n",
    "#把词语（双字）作为搭配，并通过卡方统计，选取排名前1000的词语\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "def bigram(words, score_fn=BigramAssocMeasures.chi_sq, n=1000):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  # 使用卡方统计的方法，选择排名前1000的词语\n",
    "    newBigrams = [u + v for (u, v) in bigrams]\n",
    "    #return bag_of_words(newBigrams)\n",
    "#print(bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000))\n",
    "\n",
    "# 把单个字和词语一起作为特征\n",
    "def bigram_words(words, score_fn=BigramAssocMeasures.chi_sq, n=1000):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    newBigrams = [u + v for (u, v) in bigrams]\n",
    "    a = bag_of_words(words)\n",
    "    b = bag_of_words(newBigrams)\n",
    "    a.update(b)  # 把字典b合并到字典a中\n",
    "    return a  # 所有单个字和双个字一起作为特征\n",
    "#print(bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000))\n",
    "\n",
    "import jieba\n",
    "# 返回分词列表如：[['我','爱','北京','天安门'],['你','好'],['hello']]，一条评论一个\n",
    "def readfile(filename):\n",
    "    stop=[line.strip() for line in open('stop.txt','r',\n",
    "                                        encoding='utf-8').readlines()]\n",
    "    f=open(filename,'r',encoding='utf-8')\n",
    "    line=f.readline()\n",
    "    str=[]\n",
    "    while line:\n",
    "        s=line.split('\\t')\n",
    "        fenci=jieba.cut(s[0],cut_all=False)\n",
    "        str.append(list(set(fenci)-set(stop)-set(['\\ufeff','\\n'])))\n",
    "        line=f.readline()\n",
    "    return str\n",
    "\n",
    "from nltk.probability import FreqDist,ConditionalFreqDist\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "# 获取信息量较高(前number个)的特征(卡方统计)\n",
    "def jieba_feature(number):\n",
    "    posWords=[]\n",
    "    negWords=[]\n",
    "    for items in readfile('good.txt'):\n",
    "        for item in items:\n",
    "            posWords.append(item)\n",
    "    for items in readfile('bad.txt'):\n",
    "        for item in items:\n",
    "            negWords.append(item)\n",
    "    word_fd=FreqDist()   # 可统计所有词的词频\n",
    "    con_word_fd=ConditionalFreqDist()    # 可统计积极文本中的词频和消极文本中的词频\n",
    "    for word in posWords:\n",
    "        word_fd[word]+=1\n",
    "        con_word_fd['pos'][word]+=1\n",
    "    for word in negWords:\n",
    "        word_fd[word]+=1\n",
    "        con_word_fd['neg'][word]+=1\n",
    "    pos_word_count=con_word_fd['pos'].N()    # 积极词的数量\n",
    "    neg_word_count=con_word_fd['neg'].N()    # 消极词的数量\n",
    "    # 一个词的信息量等于积极卡方统计量加上消极卡方统计量\n",
    "    total_word_count=pos_word_count+neg_word_count\n",
    "    word_scores={}\n",
    "    for word,freq in word_fd.items():\n",
    "        pos_score=BigramAssocMeasures.chi_sq(con_word_fd['pos'][word],(freq,\n",
    "                                                                       pos_word_count),total_word_count)\n",
    "        neg_score=BigramAssocMeasures.chi_sq(con_word_fd['neg'][word],(freq,\n",
    "                                                                       neg_word_count),total_word_count)\n",
    "        word_scores[word]=pos_score+neg_score\n",
    "        best_vals=sorted(word_scores.items(),key=lambda item: item[1],\n",
    "                         reverse=True)[:number]\n",
    "        best_words=set([w for w,s in best_vals])\n",
    "    return dict([(word,True) for word in best_words])\n",
    "# 构建训练需要的数据格式：\n",
    "# [[{'买': 'True', '京东': 'True', '物流': 'True', '包装': 'True', '\\n': 'True', '很快': 'True', '不错': 'True', '酒': 'True', '正品': 'True', '感觉': 'True'},  'pos'],\n",
    "# [{'买': 'True', '\\n':  'True', '葡萄酒': 'True', '活动': 'True', '澳洲': 'True'}, 'pos'],\n",
    "# [{'\\n': 'True', '价格': 'True'}, 'pos']]\n",
    "def build_features():\n",
    "    #feature = bag_of_words(text())\n",
    "    #feature = bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=900)\n",
    "    # feature =  bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=900)\n",
    "    feature = jieba_feature(1000)  # 结巴分词\n",
    "    posFeatures = []\n",
    "    for items in readfile('good.txt'):\n",
    "        a = {}\n",
    "        for item in items:\n",
    "            if item in feature.keys():\n",
    "                a[item] = 'True'\n",
    "        posWords = [a, 'pos']  # 为积极文本赋予\"pos\"\n",
    "        posFeatures.append(posWords)\n",
    "    negFeatures = []\n",
    "    for items in readfile('bad.txt'):\n",
    "        a = {}\n",
    "        for item in items:\n",
    "            if item in feature.keys():\n",
    "                a[item] = 'True'\n",
    "        negWords = [a, 'neg']  # 为消极文本赋予\"neg\"\n",
    "        negFeatures.append(negWords)\n",
    "    return posFeatures, negFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interior-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\11514\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 4.748 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "posFeatures,negFeatures=build_features()\n",
    "from random import shuffle\n",
    "shuffle(posFeatures)    #把文本的排列随机化\n",
    "shuffle(negFeatures)\n",
    "train=posFeatures[200:]+negFeatures[200:]   #只在1000条数据时合适，二八原则\n",
    "test=posFeatures[:200]+negFeatures[:200]\n",
    "data,tag=zip(*test)    #分离测试集合的数据和标签，便于测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chemical-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "def score(classifier):\n",
    "    classifier=SklearnClassifier(classifier)\n",
    "    classifier.train(train)\n",
    "    pred=classifier.classify_many(data)\n",
    "    n=0\n",
    "    s=len(pred)\n",
    "    for i in range(0,s):\n",
    "        if(pred[i]==tag[i]):\n",
    "            n=n+1\n",
    "    return n/s\n",
    "#通过实验，比较预测准确度score进而得出最佳特征提取方式、最佳特征维度、最佳分类算法\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "#print('LogisticRegression`s accuracy is  %f' % score(LogisticRegression()))\n",
    "#处理输入的评论文本，使其成为可预测格式\n",
    "def build_page(page):\n",
    "    #四中特征提取方式\n",
    "    # feature1 = bag_of_words(text())\n",
    "    # n为特征维度，可调整\n",
    "    # feature2 = bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000)\n",
    "    # feature 3=  bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=1000)\n",
    "    feature4 = jieba_feature(1000)  # 结巴分词，选取1000为特征维度，可调整\n",
    "    temp={}\n",
    "    '''\n",
    "     #单个字为特征\n",
    "     for word in page:\n",
    "         if word in feature1:\n",
    "             temp[word]='True'\n",
    "     #双字为特征\n",
    "     bigrams= BigramCollocationFinder.from_words(words)\n",
    "     text=[u + v for (u, v) in bigrams.ngram_fd]\n",
    "     for words in text:\n",
    "         if words in feature2:\n",
    "             temp[words]='True'\n",
    "     #单字和双字为特征\n",
    "     bigrams= BigramCollocationFinder.from_words(words)\n",
    "     text=[u + v for (u, v) in bigrams.ngram_fd]\n",
    "     for word in page:\n",
    "         text.append(word)\n",
    "     for words in text:\n",
    "         if words in feature3:\n",
    "             temp[words]='True'\n",
    "      '''\n",
    "    #现采用结巴分词形式处理待测文本\n",
    "    fenci0=jieba.cut(page,cut_all=False)\n",
    "    stop=[line.strip() for line in open('stop.txt','r',\n",
    "                                        encoding='utf-8').readlines()]   #停用词\n",
    "    for words in list(set(fenci0)-set(stop)):\n",
    "        if words in feature4:\n",
    "            temp[words]='True'\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sublime-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将实验比较得出的最佳分类算法（classifier_ag）构造的分类器保存\n",
    "def classfier_model(classifier_ag):\n",
    "    classifier = SklearnClassifier(classifier_ag)\n",
    "    classifier.train(train)\n",
    "    return classifier\n",
    "#假设逻辑回归为最佳分类算法\n",
    "#classifier=classfier_model(classifier_ag=LogisticRegression)\n",
    "#用最佳分类器预测待测文本\n",
    "def predict_page(page):\n",
    "    pred = classifier.classify_many(data)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focal-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对users中的每条评论进行情感判断\n",
    "def emotion_decide():\n",
    "    results=collection1.find()\n",
    "    for result in results:\n",
    "        if result['page']:   #若评论为空，则不进行情感分析\n",
    "           condition={'page':result['page']}\n",
    "           if collection1.update_many(condition,{'$set':{\n",
    "               'emotion':emotional.predict_page(page=result['page'])}}):\n",
    "               print(\"successful\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
